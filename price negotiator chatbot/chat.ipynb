{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10e803c-315b-4c16-a776-43fd2a47978e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhan Khan\\AppData\\Local\\Temp\\ipykernel_23320\\3010357087.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to see you here\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how much amoutn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have a nice day\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how much amount\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not understand...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how much amountfree\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See you later, thanks for visiting\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  free\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we do not keep such policies\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  give me a product\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your offer?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from model import NeuralNet\n",
    "from nltk_utils import bag_of_words, tokenize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('D:/chatbot_project3/chatbot-deployment-main/intent.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"D:/chatbot_project3/chatbot-deployment-main/data_Nego.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Sam\"\n",
    "\n",
    "def get_response(msg):\n",
    "    sentence = tokenize(msg)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                return random.choice(intent['responses'])\n",
    "    \n",
    "    return \"I do not understand...\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Let's chat! (type 'quit' to exit)\")\n",
    "    while True:\n",
    "        # sentence = \"do you use credit cards?\"\n",
    "        sentence = input(\"You: \")\n",
    "        if sentence == \"quit\":\n",
    "            break\n",
    "\n",
    "        resp = get_response(sentence)\n",
    "        print(resp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83efb1da-8d5e-4792-8014-b1d393c169e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhan Khan\\AppData\\Local\\Temp\\ipykernel_23320\\3819541830.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(FILE)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from model import NeuralNet\n",
    "from nltk_utils import bag_of_words, tokenize\n",
    "\n",
    "# Load the chatbot model and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('D:/chatbot_project3/chatbot-deployment-main/intent.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"D:/chatbot_project3/chatbot-deployment-main/data_Nego.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Sam\"\n",
    "\n",
    "def get_response(msg):\n",
    "    sentence = tokenize(msg)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                return random.choice(intent['responses'])\n",
    "    \n",
    "    return \"I do not understand...\"\n",
    "\n",
    "# Define the UI using Tkinter\n",
    "def send():\n",
    "    user_input = entry.get()\n",
    "    chat_area.configure(state=tk.NORMAL)\n",
    "    chat_area.insert(tk.END, f\"You: {user_input}\\n\")\n",
    "    entry.delete(0, tk.END)\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        root.destroy()\n",
    "        return\n",
    "    \n",
    "    response = get_response(user_input)\n",
    "    chat_area.insert(tk.END, f\"{bot_name}: {response}\\n\")\n",
    "    chat_area.configure(state=tk.DISABLED)\n",
    "    chat_area.yview(tk.END)\n",
    "\n",
    "# Initialize the Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "# Chat area with a scrollbar\n",
    "chat_area = scrolledtext.ScrolledText(root, wrap=tk.WORD)\n",
    "chat_area.configure(state=tk.DISABLED)\n",
    "chat_area.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Entry field for user input\n",
    "entry = tk.Entry(root, width=80)\n",
    "entry.pack(padx=20, pady=10)\n",
    "\n",
    "# Button to send the message\n",
    "send_button = tk.Button(root, text=\"Send\", command=send)\n",
    "send_button.pack(pady=5)\n",
    "\n",
    "# Bind the Enter key to the send function\n",
    "root.bind('<Return>', lambda event: send())\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ee2050-f8f1-4e30-9f76-2111614c3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhan Khan\\AppData\\Local\\Temp\\ipykernel_23320\\3408452016.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(FILE)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import mysql.connector\n",
    "from model import NeuralNet\n",
    "from nltk_utils import bag_of_words, tokenize\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",  # Replace with your MySQL username\n",
    "    password=\"\",  # Replace with your MySQL password\n",
    "    database=\"chatbot\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load the chatbot model and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('D:/chatbot_project3/chatbot-deployment-main/intent.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"D:/chatbot_project3/chatbot-deployment-main/data_Nego.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Sam\"\n",
    "\n",
    "def get_response(msg):\n",
    "    sentence = tokenize(msg)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                return random.choice(intent['responses'])\n",
    "    \n",
    "    return \"I do not understand...\"\n",
    "\n",
    "# Function to save conversation to the database\n",
    "def save_to_db(user_input, bot_response):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    query = \"INSERT INTO interactions (user_input, bot_response, timestamp) VALUES (%s, %s, %s)\"\n",
    "    cursor.execute(query, (user_input, bot_response, timestamp))\n",
    "    conn.commit()\n",
    "\n",
    "# Define the UI using Tkinter\n",
    "def send():\n",
    "    user_input = entry.get()\n",
    "    chat_area.configure(state=tk.NORMAL)\n",
    "    chat_area.insert(tk.END, f\"You: {user_input}\\n\")\n",
    "    entry.delete(0, tk.END)\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        root.destroy()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    response = get_response(user_input)\n",
    "    chat_area.insert(tk.END, f\"{bot_name}: {response}\\n\")\n",
    "    chat_area.configure(state=tk.DISABLED)\n",
    "    chat_area.yview(tk.END)\n",
    "    \n",
    "    # Save the conversation to the database\n",
    "    save_to_db(user_input, response)\n",
    "\n",
    "# Initialize the Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "# Chat area with a scrollbar\n",
    "chat_area = scrolledtext.ScrolledText(root, wrap=tk.WORD)\n",
    "chat_area.configure(state=tk.DISABLED)\n",
    "chat_area.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Entry field for user input\n",
    "entry = tk.Entry(root, width=80)\n",
    "entry.pack(padx=20, pady=10)\n",
    "\n",
    "# Button to send the message\n",
    "send_button = tk.Button(root, text=\"Send\", command=send)\n",
    "send_button.pack(pady=5)\n",
    "\n",
    "# Bind the Enter key to the send function\n",
    "root.bind('<Return>', lambda event: send())\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpu",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
